{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set 3\n",
    "\n",
    "**Total points**: 24\n",
    "\n",
    "**Background on two data sets**: here, we're going to use two datasets to practice regex patterns and merging/matching. Both datasets relate to the broader issue of which employers might be violating the rights of temporary guestworkers, with more details on the policy background available at last spring's course page under the \"Social impact practicum context\" header: https://rebeccajohnson88.github.io/qss20/docs/sip_finalproject.html\n",
    "\n",
    "\n",
    "The following datasets are located in `pset3_inputdata` (need to unzip): \n",
    "\n",
    "- `jobs`: a dataset of guestworker jobs posted by many employers, some of whom have been debarred from the program for labor abuses; others not debarred\n",
    "- `debar`: a dataset of employers who committed violations of labor regulations meant to protect temporary guestworkers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helpful packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import recordlinkage\n",
    "\n",
    "\n",
    "## repeated printouts\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regex and exact matching (10 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Load data on debarments and job postings (0 points)\n",
    "\n",
    "Load the following datasets stored in `pset3_inputdata`\n",
    "    \n",
    "- Historical H2A debarments (debar.csv); call this `debar`\n",
    "- Q1 2021 H2A job postings (jobs.csv); call this `jobs`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to load the data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Try exact merge on business name  (2 points)\n",
    "\n",
    "- Use the `EMPLOYER_NAME` field of the `jobs` dataset\n",
    "- Use the `Name` field of the `debar` dataset \n",
    "\n",
    "A. Use pd.merge with an inner join on those fields to see whether there are any exact matches. \n",
    "\n",
    "B. If there are exact matches, subset to the following columns and print the rows with exact matches:\n",
    "\n",
    "-`Employer_NAME` and `Name` \n",
    "- Date range of debarment (`Start date` and `End date` in `debar`)\n",
    "- Location from each data (`City, State` in `debar` and `EMPLOYER_CITY` and `EMPLOYER_STATE` in `jobs`)\n",
    "\n",
    "**Concepts tested and resources**: part A tests using `pd.merge` for exact merging; part B tests column subsetting \n",
    "   - Slides on exact merging: https://github.com/rebeccajohnson88/qss20_slides_activities/blob/main/slides/w22_slides/05_qss20_w22_unit5_mergingexact.pdf \n",
    "   - Code with examples of exact merging: https://github.com/rebeccajohnson88/qss20_slides_activities/blob/main/activities/w22_activities/solutions/03_merging_exact_solutions.ipynb \n",
    "   - Code with example of column subsetting: https://github.com/rebeccajohnson88/qss20_slides_activities/blob/main/activities/w22_activities/solutions/00_pandas_datacleaning_solutions.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to do the merge and printing of exact match(es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Targeted regex\n",
    "\n",
    "You want to see if you can increase the exact match rate with some basic cleaning of each \n",
    "of the employer name fields in each dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Converting to upper (2 points)\n",
    "\n",
    "A. Convert the `EMPLOYER_NAME` and `Name` fields to uppercase using list comprehension rather than df.varname.str.upper() (it's fine to do a separate list comprehension line for each of the two columns)\n",
    "\n",
    "B. Print a random sample of 15 values of each result\n",
    "\n",
    "C. Assign the full vector of uppercase names back to the original data, writing over the original `EMPLOYER_NAME` and `Name` columns \n",
    "\n",
    "**Resources**:\n",
    "    - Activity code with list comprehension: https://github.com/rebeccajohnson88/qss20_slides_activities/blob/main/activities/w22_activities/solutions/00_pandas_datacleaning_solutions.ipynb \n",
    "    - Sampling from a list without replacement using the `random` module: https://note.nkmk.me/en/python-random-choice-sample-choices/ \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert your code to turn into uppercase here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert your code for the random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert your code for assigning the uppercase names back to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Cleaning up punctuation (6 points)\n",
    "\n",
    "You notice that INC, CO, and LLC are sometimes followed by a period (.) but sometimes not\n",
    "\n",
    "A. For each dataset, write a regex pattern using `re.sub` to remove the . but only if it's preceded by INC, LLC, or CO \n",
    "\n",
    "Make sure LLC, INC, CO remain part of the string but just without the dot\n",
    "\n",
    "B. Test the pattern on the positive and negative example we provide below and print the result. See the Github issue for examples of what to return\n",
    "\n",
    "\n",
    "**Hint**: https://stackoverflow.com/questions/7191209/python-re-sub-replace-with-matched-content\n",
    "\n",
    "**Resources**:\n",
    "    - Regex slides: https://github.com/rebeccajohnson88/qss20_slides_activities/blob/main/slides/w22_slides/06_qss20_w22_unit6_regex.pdf \n",
    "    - Regex activity code: https://github.com/rebeccajohnson88/qss20_slides_activities/blob/main/activities/w22_activities/solutions/04_basicregex_solutions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_example_1 = \"CISCO PRODUCE INC.\"\n",
    "pos_example_2 = \"AVOYELLES HONEY CO., LLC\"\n",
    "neg_example = \"E.V. RANCH LLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert your code here with the regex pattern for part A\n",
    "\n",
    "## insert your code to use re.sub to apply the pattern to the test cases for part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Use that pattern in conjunction with `re.sub` and list comprehension to clean the columns in each dataset. Save the new columns as `name_clean` in each. Then, use row subsetting to (1) subset to rows that changed names and (2) for:\n",
    "\n",
    "- `debar` print the `Name` and `name_clean` columns\n",
    "- `jobs` print the `EMPLOYER_NAME` and `name_clean` columns\n",
    "\n",
    "Make sure to use the uppercase versions of the variables\n",
    "\n",
    "**Concepts and resources**: for the last part of row subsetting and printing two columns in each dataframe, use the `.loc` examples shown here: https://github.com/rebeccajohnson88/qss20_slides_activities/blob/main/activities/w22_activities/solutions/00_pandas_datacleaning_solutions.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here to clean the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here to print the head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional extra credit (1.3.3) regex to separate companies from individuals (4 points)\n",
    "\n",
    "You notice some employers in `debar` have both the name of the company and the name of individual, e.g.:\n",
    "    \n",
    "COUNTY FAIR FARM (COMPANY) AND ANDREW WILLIAMSON (INDIVIDUAL)*\n",
    "\n",
    "Use the uppercase/cleaned `name_clean` in `debar`\n",
    "\n",
    "A. Write a regex pattern that does the following:\n",
    "    - Captures the pattern that occurs before COMPANY if (COMPANY) is in string; so in example above, extracts COUNTY FAIR FARM \n",
    "    - Captures the pattern that occurs before INDIVIDUAL if (INDIVIDUAL) is also in string -- so in above, extracts ANDREW WILLIAMSON (so omit the \"and\")\n",
    "    \n",
    "B. Test the pattern on `pos_example` and `neg_example`-- make sure former returns a list (if using find.all) or match object (if using re.search) with the company name and individual name separated out; make sure latter returns empty\n",
    "    \n",
    "**Hints and resources**: for step A, you can either use re.search, re.match, or re.findall; don't worry about matching B&R Harvesting and Paul Cruz (Individual)\n",
    "\n",
    "- Same regex resources as above\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_example = \"COUNTY FAIR FARM (COMPANY) AND ANDREW WILLIAMSON (INDIVIDUAL)*\"\n",
    "neg_example = \"CISCO PRODUCE INC\"\n",
    "\n",
    "## your code here to define the pattern\n",
    "\n",
    "## your code here to apply it to the pos_example\n",
    "\n",
    "## your code here to apply it to the negative example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Iterate over the `name_clean` column in debar and use regex to create two new columns in `debar`:\n",
    "   - `co_name`: A column for company (full `name_clean` string if no match; pattern before COMPANY if one extracted)\n",
    "   - `ind_name`: A column for individual (full `name_clean` string if no match; pattern before INDIVIDUAL if one extracted)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here to create these columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "D. Print three columns for the rows in `debar` containing the negative example and positive example described above (county fair farm and cisco produce):\n",
    "\n",
    "- `name_clean`\n",
    "- `co_name`\n",
    "- `ind_name`\n",
    "- `Violation`\n",
    "\n",
    "**Note**: as shown in the outcome there may be duplicates of the same company reflecting different violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to print these columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Fuzzy matching to match debarments to jobs (14 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocessing state names (2 points)\n",
    "\n",
    "You want to block on state but notice that states in `debar` have a mix of two digit codes and full state names (e.g., GA versus Georgia) while states in `jobs` are all two-digit state codes\n",
    "\n",
    "A. Run the code below to load the `states` crosswalk (this matches state abbreviations with their full name). \n",
    "\n",
    "B. Use that crosswalk to create a field in `debar` that has the two-digit state abbreviation for all locations (hint: you may need to first split the `City, State` string on the \", \" or use str.replace to extract the state into a new column before converting it to all abbreviations)\n",
    "\n",
    "**Hint**: the GitHub issue contains the value_counts() for the cleaned two-digit states after this step\n",
    "\n",
    "C. Use an `assert` statement to check that `all` the states in `debar` are two-digits after the cleaning\n",
    "\n",
    "**Notes**: you can filter out states that are NaN\n",
    "\n",
    "**Concepts and resources**:\n",
    "\n",
    "- For part B, draw on regex or pandas str functions to create a `State` variable from the `City, State` column in `debar`\n",
    "- Then, one approach is to use `pd.merge` to merge the state crosswalk onto debar (example head() shown in the GitHub issue)\n",
    "- You may want to structure the `pd.merge` to only merge on rows *without* two-digit names to the crosswalk with those two-digit names and `pd.concat` to bind the dataframe back together\n",
    "- How to structure an assert statement: https://www.w3schools.com/python/ref_keyword_assert.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code to load state crosswalk\n",
    "## source- https://towardsdatascience.com/state-name-to-state-abbreviation-crosswalks-6936250976c\n",
    "cw_location = 'http://app02.clerk.org/menu/ccis/Help/CCIS%20Codes/'\n",
    "cw_filename = 'state_codes.html'\n",
    "states = pd.read_html(cw_location + cw_filename)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here to add two-digit state codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 step by step fuzzy matching (4 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Write fuzzy matching code (don't yet put inside a user-defined function, you'll do that in 2.3) that:\n",
    "\n",
    "- Blocks on two-digit state code\n",
    "- Finds matches based on similarity between the employer name (`name_clean`) in `debar` (uppercase and cleaned) and `name_clean` in `jobs` (uppercase and cleaned). You can choose which distance metric and threshold to use (feel free to set a threshold low enough to get some matches even if that leads to some false positives).\n",
    "\n",
    "For the steps after compute, just take any match with non-zero value rather than using a classifier (so skip the k-means or e-m step from the class example code)\n",
    "\n",
    "**Concepts and resources**:\n",
    "\n",
    "- Solutions code here (more consolidated): https://github.com/rebeccajohnson88/qss20_slides_activities/blob/main/activities/w22_activities/solutions/05_merging_fuzzy_activity_solutions.ipynb\n",
    "- Example code here (more step by step): https://github.com/rebeccajohnson88/qss20_slides_activities/blob/main/activities/w22_activities/solutions/05_merging_fuzzy_codeexample.ipynb \n",
    "\n",
    "**Hint**: you may need to deduplicate records in the datasets for the recordlinkage package to work. See drop_duplicates within pandas; the `subset` command within `drop_duplicates` allows you to only consider certain columns for duplicates; drop based on duplicates in the two-digit state code and `name_clean`. After you drop_duplicates, you may need to use reset_index to create a new index in the data, eg:\n",
    "\n",
    "`dedup_df = df.drop_duplicates(subset = ['state', 'name_clean']).reset_index(drop = True)`\n",
    "\n",
    "`dedup_df['df_ind'] = dedup_df.index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your fuzzy matching code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "B. Print the following columns in the resulting matched dataset and comment on examples of ones that seem like true positive matches and ones that seem like false positive matches:\n",
    "\n",
    "- `name_clean` for jobs and debar\n",
    "- `state` in debar\n",
    "- `state` in jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Put the code from 2.2 into a fuzzy matching function that generalizes some steps (8 points)\n",
    "\n",
    "You want to see how the matches change if you add the city and not only state as a field and also want to automate the process of matching a bit to try different distance thresholds.\n",
    "\n",
    "A. Extract the City from the `City, State` column of `debar`\n",
    "\n",
    "B. Convert that new `city` column to uppercase and convert the `EMPLOYER_CITY` column in `jobs` to uppercase\n",
    "\n",
    "C. Write a function surrounding the code in `recordlinkage` that you wrote in problem 2.2 (so you don't need to recode the package from scratch) that (1) takes in each dataset, (2) blocks on two-digit state, and (3) fuzzy matches on employer name and employer city. See below notes on partial versus full credit for function structure. \n",
    "\n",
    "D. Execute the function with a couple different string distance thresholds and print the resulting matches for each\n",
    "\n",
    "6 out of 8 points: function takes arguments for input datasets, varname to block on, two varnames to fuzzy match on (`name_clean` and `city)`, string distance function, and string distance threshold\n",
    "    \n",
    "8 out of 8: above but function is also general enough that it takes a variable # of strings to match on--- so should work if you either execute just using employer name or also work if you execute using employer name and employer city as the fuzzy variables.\n",
    "\n",
    "**Concepts and resources**: same as above. One hint is that for us, the easiest way to get to the full credit solution was to feed the function a dictionary where each key corresponds to one string variable to fuzzy match on; the values contain that variable's name in each dataset, the distance metric, and the distance threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here to create the city variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here to define the fuzzy matching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here to execute it with a couple different string distance thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optional extra credit (up to 6 points)\n",
    "\n",
    "- For 2 points extra credit, create a visualization with 1+ of the existing fields in either the raw `jobs` or `debar` data. We'll be showing cool visualizations in class so use your imagination! Options could include visualizing between-state or over-time variation\n",
    "\n",
    "- For 6 points extra credit instead, geocode the employer addresses in `jobs` and plot the addresses of jobs as points overlaid on top of a map of Georgia \n",
    "    - **Note**: this extra credit involves Googling since we have not yet covered spatial data. \n",
    "        - For discussion of how to geocode addresses -> lat/long, see: https://www.natasshaselvaraj.com/a-step-by-step-guide-on-geocoding-in-python/ \n",
    "        - For discussion of plotting lat/long dots against a map, see this discussion of geopandas: https://towardsdatascience.com/plotting-maps-with-geopandas-428c97295a73\n",
    "    - Relevant columns include `EMPLOYER_ADDRESS_1` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
